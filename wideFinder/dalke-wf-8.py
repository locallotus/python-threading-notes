# dalke-wf-8.py
# A version using mxTextTools 3.0, with the tag table generated by Martel
# http://www.dalkescientific.com/writings/diary/archive/2007/10/07/wide_finder.html
'''
mxTextTools, with regexps via Martel

Now that I have fast baseline code, I want to compare it to mxTextTools 3.0. There is no regular expression support in mxTextTools.
Instead, you define a tag table, which are machine instructions for the tag engine.
While there is some documentation on how to use mxTextTools, it's not easy to understand.
The examples which come with mxTextTools 3.0 aren't that helpful, and some of them haven't even been updated to work with 3.0.

Some years back I spent some time working on Martel, which uses an (almost) regular expression language as a way to parse semi-structured flat-files as if it's in XML.
The element definitions comes from group names in the regular expression, and the text parsing is done with mxTextTools.
I wrote some code to have Martel convert Tim's filter regular expression into a tag table.
I did change the regexp so that the group has a name, as group names are used as tag in the tag table.
'''

from collections import defaultdict
from mx.TextTools import *

import Martel


FILE = "o1000k.ap"

import time, sys
if sys.platform == "win32":
    timer = time.clock
else:
    timer = time.time

t0, t1 = timer(), time.clock()

# My Martel package implements a converter from regular expressions to
# mxTextTool tag table, but it doesn't take full advantage of mxTextTools.
expr = Martel.Re("GET /ongoing/When/\d\d\dx/(?P<page>\d\d\d\d/\d\d/\d\d/[^ .]+) ")
tag_table = ("start",
             # For each character, see if it is parsed by the regular expression
             #   if no, go to the next state, if yes go to the "start" tag
             (None, SubTable, expr.make_parser().tagtable, +1, "start"),
             # At end of file?  If no, go to the next state, if yes, stop parsing and declare victory
             (None, EOF, 0, +1, MatchOk),
             # Advance to the next character and start again
             (None, Skip, 1, "start", "start"),
             )
tag_table = TagTable(tag_table)

count = defaultdict(int)

CHUNK = 12*1024

infile = open(FILE)

while 1:
    # Guarantee the complete line is read
    text = infile.read(CHUNK)
    text += infile.readline()
    if not text:
        break

    success, taglist, position = tag(text, tag_table)
    for (_, l, r, _) in taglist:
        count[text[l:r]] += 1

for key in sorted(count, key=count.get)[:10]:
    pass # print "%40s = %s" % (key, count[key])

print timer() - t0, time.clock() - t1

# sanity check
for key in sorted(count, key=count.get)[-10:]:
    print "%40s = %s" % (key, count[key])


'''
Bad news. It's REALLY SLOW. It takes about 26s to process the log file, compared to the under 2s using the normal approach and 1.3s for my best single-threaded code.

What's wrong? Well, one thing is that my Martel code is meant as a validator.
It assumes most characters will match and it's very bad at searching for a match because it checks the text character-by-character.
More sophisticated algorithms, like Boyer-Moore, can analyze the search string and skip testing characters when there's no chance of a match.

For example, the search string "GET /ongoing/When/" only contains the characters " /EGTWeghino".
If neither text[i] nor text[i+18] contain a character in that set then it's impossible for that search string to exist in that range.
Python's find, index and x in s tests use a smarter superlinear substring search algorithm, which Fredrik uses as a first-level filter in his wf-2.py code.
'''